{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers.core import Dense\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper function to pickle and load stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def __pickleStuff(filename, stuff):\n",
    "    save_stuff = open(filename, \"wb\")\n",
    "    pickle.dump(stuff, save_stuff)\n",
    "    save_stuff.close()\n",
    "def __loadStuff(filename):\n",
    "    saved_stuff = open(filename,\"rb\")\n",
    "    stuff = pickle.load(saved_stuff)\n",
    "    saved_stuff.close()\n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "**phrases_embed.csv** came from [Babylon blog \"How the chatbot understands sentences\"](https://blog.babylonhealth.com/how-the-chatbot-understands-sentences-fe6c5deb6e81).\n",
    "\n",
    "Checkout the data visualization [here](http://s3-eu-west-1.amazonaws.com/nils-demo/phrases.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Disease</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stomach ache</td>\n",
       "      <td>abdomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am having stomach pains with diahorrea and t...</td>\n",
       "      <td>abdomen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stomach pain</td>\n",
       "      <td>abdomen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Disease    class\n",
       "0                                       stomach ache  abdomen\n",
       "1  I am having stomach pains with diahorrea and t...  abdomen\n",
       "2                                       stomach pain  abdomen"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_csv(\"./data/phrases_embed.csv\")\n",
    "df = df[[\"Disease\", \"class\"]]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get those two columns to numpy arrays pairs\n",
    "\"Disease\" columns ==> documents\n",
    "\n",
    "\"class\" columns ==> body_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents.shape: (1261,)\n",
      "body_positions.shape: (1261,)\n"
     ]
    }
   ],
   "source": [
    "documents=df.as_matrix(columns=df.columns[0:1])\n",
    "documents = documents.reshape(documents.shape[0])\n",
    "print(\"documents.shape: {}\".format(documents.shape))\n",
    "body_positions=df.as_matrix(columns=df.columns[1:])\n",
    "body_positions = body_positions.reshape(body_positions.shape[0])\n",
    "print(\"body_positions.shape: {}\".format(body_positions.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the data\n",
    "Function to clean up data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_special_chars = re.compile(\"[^A-Za-z0-9 ]+\")\n",
    "def cleanUpSentence(r, stop_words = None):\n",
    "    r = r.lower().replace(\"<br />\", \" \")\n",
    "    r = re.sub(strip_special_chars, \"\", r.lower())\n",
    "    if stop_words is not None:\n",
    "        words = word_tokenize(r)\n",
    "        filtered_sentence = []\n",
    "        for w in words:\n",
    "            if w not in stop_words:\n",
    "                filtered_sentence.append(w)\n",
    "        return \" \".join(filtered_sentence)\n",
    "    else:\n",
    "        return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we are cleaning up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totalX = []\n",
    "totalY = []\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "for i, doc in enumerate(documents):\n",
    "    totalX.append(cleanUpSentence(doc, stop_words))\n",
    "    body_positions[i] = re.sub(strip_special_chars, \"\", body_positions[i].lower())\n",
    "    totalY.append(body_positions[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show input max sequence length\n",
    "If the max input sequence length is too long, we can put a limit to it in order to reduce the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max input length is:  18\n"
     ]
    }
   ],
   "source": [
    "xLengths = [len(word_tokenize(x)) for x in totalX]\n",
    "h = sorted(xLengths)  #sorted lengths\n",
    "maxLength =h[len(h)-1]\n",
    "print(\"max input length is: \",maxLength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert input words to ids\n",
    "**max_vocab_size**: the maximum number of words to keep, we choose 30000 since it is big enough to keep all words in this case.\n",
    "\n",
    "Pad each input sequence to max input length **maxLength** if it is shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_vocab_size: 910\n"
     ]
    }
   ],
   "source": [
    "max_vocab_size = 30000\n",
    "input_tokenizer = Tokenizer(max_vocab_size)\n",
    "input_tokenizer.fit_on_texts(totalX)\n",
    "input_vocab_size = len(input_tokenizer.word_index) + 1\n",
    "print(\"input_vocab_size:\",input_vocab_size)\n",
    "totalX = np.array(pad_sequences(input_tokenizer.texts_to_sequences(totalX), maxlen=maxLength))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at one sequence in **totalX**, sequence has length 18, each number here represent a unique word. \"0\" is padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        41,  40, 465, 206,  29])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalX[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the input tokenizer\n",
    "Since we need to use the same tokenizer for predition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "__pickleStuff(\"./data/input_tokenizer.p\",input_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert output words to ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abdomen', 'abdomen', 'abdomen']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalY[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tokenizer = Tokenizer(30)\n",
    "target_tokenizer.fit_on_texts(totalY)\n",
    "target_vocab_size = len(target_tokenizer.word_index) + 1\n",
    "totalY = np.array(target_tokenizer.texts_to_sequences(totalY)) -1\n",
    "totalY = totalY.reshape(totalY.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_vocab_size: 19\n"
     ]
    }
   ],
   "source": [
    "print(\"target_vocab_size:\",target_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalY[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turn output to categories(one-hot vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "totalY = to_categorical(totalY, num_classes=target_vocab_size) # turn output to one-hot vecotrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "         0.,  0.,  0.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalY[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = input_vocab_size # vocab_size for model word embeding input\n",
    "output_dimen = totalY.shape[1] # number of unique output classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**target_reverse_word_index** to turn class ids to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abdomen'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_reverse_word_index = {v: k for k, v in list(target_tokenizer.word_index.items())}\n",
    "target_reverse_word_index[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save meta data for later predition\n",
    "maxLength: the input sequence length\n",
    "\n",
    "vocab_size: Input vocab size\n",
    "\n",
    "output_dimen: number of unique output classes\n",
    "\n",
    "target_reverse_word_index: turn predicted class ids to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "metaData = {\"maxLength\":maxLength,\"vocab_size\":vocab_size,\"output_dimen\":output_dimen,\"target_reverse_word_index\":target_reverse_word_index}\n",
    "__pickleStuff(\"./data/metaData_triage.p\", metaData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model, train and save it\n",
    "The training data is logged to Tensorboard, we can look at it by cd into directory \n",
    "\n",
    "\"./Graph/medical_triage\" and run\n",
    "\n",
    "\n",
    "\"python -m tensorflow.tensorboard --logdir=.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 18, 256)           232960    \n",
      "_________________________________________________________________\n",
      "gru_5 (GRU)                  (None, 18, 256)           393984    \n",
      "_________________________________________________________________\n",
      "gru_6 (GRU)                  (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 19)                4883      \n",
      "=================================================================\n",
      "Total params: 1,025,811\n",
      "Trainable params: 1,025,811\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1134 samples, validate on 127 samples\n",
      "Epoch 1/40\n",
      "1134/1134 [==============================] - 1s - loss: 2.8194 - acc: 0.1049 - val_loss: 2.9417 - val_acc: 0.0630\n",
      "Epoch 2/40\n",
      "1134/1134 [==============================] - 1s - loss: 2.7203 - acc: 0.1402 - val_loss: 2.9489 - val_acc: 0.0709\n",
      "Epoch 3/40\n",
      "1134/1134 [==============================] - 1s - loss: 2.6869 - acc: 0.1455 - val_loss: 2.9797 - val_acc: 0.0551\n",
      "Epoch 4/40\n",
      "1134/1134 [==============================] - 1s - loss: 2.6428 - acc: 0.1614 - val_loss: 3.0163 - val_acc: 0.0551\n",
      "Epoch 5/40\n",
      "1134/1134 [==============================] - 1s - loss: 2.6027 - acc: 0.1799 - val_loss: 2.8927 - val_acc: 0.0945\n",
      "Epoch 6/40\n",
      "1134/1134 [==============================] - 1s - loss: 2.5015 - acc: 0.2152 - val_loss: 2.8417 - val_acc: 0.0866\n",
      "Epoch 7/40\n",
      "1134/1134 [==============================] - 1s - loss: 2.3432 - acc: 0.2840 - val_loss: 2.7341 - val_acc: 0.1339\n",
      "Epoch 8/40\n",
      "1134/1134 [==============================] - 1s - loss: 2.0700 - acc: 0.3739 - val_loss: 2.4322 - val_acc: 0.2835\n",
      "Epoch 9/40\n",
      "1134/1134 [==============================] - 1s - loss: 1.7288 - acc: 0.4674 - val_loss: 2.1770 - val_acc: 0.3228\n",
      "Epoch 10/40\n",
      "1134/1134 [==============================] - 1s - loss: 1.5467 - acc: 0.5326 - val_loss: 1.8550 - val_acc: 0.5276\n",
      "Epoch 11/40\n",
      "1134/1134 [==============================] - 1s - loss: 1.3716 - acc: 0.5864 - val_loss: 1.8085 - val_acc: 0.5354\n",
      "Epoch 12/40\n",
      "1134/1134 [==============================] - 1s - loss: 1.2504 - acc: 0.6208 - val_loss: 1.7382 - val_acc: 0.5197\n",
      "Epoch 13/40\n",
      "1134/1134 [==============================] - 1s - loss: 1.0641 - acc: 0.6631 - val_loss: 1.5692 - val_acc: 0.6299\n",
      "Epoch 14/40\n",
      "1134/1134 [==============================] - 1s - loss: 1.0097 - acc: 0.6764 - val_loss: 1.4859 - val_acc: 0.5512\n",
      "Epoch 15/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.9388 - acc: 0.7072 - val_loss: 1.4094 - val_acc: 0.6850\n",
      "Epoch 16/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.9006 - acc: 0.7160 - val_loss: 1.3329 - val_acc: 0.6850\n",
      "Epoch 17/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.8017 - acc: 0.7504 - val_loss: 1.3012 - val_acc: 0.7165\n",
      "Epoch 18/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.7370 - acc: 0.7654 - val_loss: 1.1721 - val_acc: 0.7244\n",
      "Epoch 19/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.7234 - acc: 0.7663 - val_loss: 1.1755 - val_acc: 0.7244\n",
      "Epoch 20/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.6869 - acc: 0.7760 - val_loss: 1.0929 - val_acc: 0.7323\n",
      "Epoch 21/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.5710 - acc: 0.8131 - val_loss: 1.1193 - val_acc: 0.7402\n",
      "Epoch 22/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.5542 - acc: 0.8201 - val_loss: 1.0612 - val_acc: 0.7638\n",
      "Epoch 23/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.5207 - acc: 0.8413 - val_loss: 1.0734 - val_acc: 0.7323\n",
      "Epoch 24/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.4838 - acc: 0.8527 - val_loss: 1.0481 - val_acc: 0.7717\n",
      "Epoch 25/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.4502 - acc: 0.8439 - val_loss: 1.2064 - val_acc: 0.7795\n",
      "Epoch 26/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.4348 - acc: 0.8695 - val_loss: 1.2019 - val_acc: 0.7402\n",
      "Epoch 27/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.3803 - acc: 0.8871 - val_loss: 1.1104 - val_acc: 0.8031\n",
      "Epoch 28/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.4317 - acc: 0.8642 - val_loss: 1.1321 - val_acc: 0.7717\n",
      "Epoch 29/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.3763 - acc: 0.8695 - val_loss: 1.0240 - val_acc: 0.8031\n",
      "Epoch 30/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.3175 - acc: 0.8995 - val_loss: 1.1261 - val_acc: 0.8110\n",
      "Epoch 31/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.3504 - acc: 0.8933 - val_loss: 1.3131 - val_acc: 0.7402\n",
      "Epoch 32/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.3330 - acc: 0.9056 - val_loss: 1.1257 - val_acc: 0.7953\n",
      "Epoch 33/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.3677 - acc: 0.8783 - val_loss: 1.0425 - val_acc: 0.8031\n",
      "Epoch 34/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.2462 - acc: 0.9153 - val_loss: 1.1284 - val_acc: 0.7953\n",
      "Epoch 35/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.3082 - acc: 0.9074 - val_loss: 1.0674 - val_acc: 0.7953\n",
      "Epoch 36/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.2679 - acc: 0.9065 - val_loss: 1.1294 - val_acc: 0.7717\n",
      "Epoch 37/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.2658 - acc: 0.9153 - val_loss: 1.3375 - val_acc: 0.7717\n",
      "Epoch 38/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.2610 - acc: 0.9153 - val_loss: 1.1011 - val_acc: 0.8110\n",
      "Epoch 39/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.2584 - acc: 0.9065 - val_loss: 1.2811 - val_acc: 0.8031\n",
      "Epoch 40/40\n",
      "1134/1134 [==============================] - 1s - loss: 0.2061 - acc: 0.9224 - val_loss: 1.2347 - val_acc: 0.7953\n",
      "Saved model!\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 256\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim,input_length = maxLength))\n",
    "# Each input would have a size of (maxLengthx256) and each of these 256 sized vectors are fed into the GRU layer one at a time.\n",
    "# All the intermediate outputs are collected and then passed on to the second GRU layer.\n",
    "model.add(GRU(256, dropout=0.9, return_sequences=True))\n",
    "# Using the intermediate outputs, we pass them to another GRU layer and collect the final output only this time\n",
    "model.add(GRU(256, dropout=0.9))\n",
    "# The output is then sent to a fully connected layer that would give us our final output_dim classes\n",
    "model.add(Dense(output_dimen, activation='softmax'))\n",
    "# We use the adam optimizer instead of standard SGD since it converges much faster\n",
    "tbCallBack = TensorBoard(log_dir='./Graph/medical_triage', histogram_freq=0,\n",
    "                            write_graph=True, write_images=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(totalX, totalY, validation_split=0.1, batch_size=32, epochs=40, verbose=1, callbacks=[tbCallBack])\n",
    "model.save('./data/triage.HDF5')\n",
    "\n",
    "print(\"Saved model!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below prediction code\n",
    "Function to load the meta data and the model we just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = None\n",
    "target_reverse_word_index = None\n",
    "maxLength = 0\n",
    "def loadModel():\n",
    "    global model, target_reverse_word_index, maxLength\n",
    "    metaData = __loadStuff(\"./data/metaData_triage.p\")\n",
    "    maxLength = metaData.get(\"maxLength\")\n",
    "    vocab_size = metaData.get(\"vocab_size\")\n",
    "    output_dimen = metaData.get(\"output_dimen\")\n",
    "    target_reverse_word_index = metaData.get(\"target_reverse_word_index\")\n",
    "    embedding_dim = 256\n",
    "    if model is None:\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, embedding_dim, input_length=maxLength))\n",
    "        # Each input would have a size of (maxLength x 256) and each of these 256 sized vectors are fed into the GRU layer one at a time.\n",
    "        # All the intermediate outputs are collected and then passed on to the second GRU layer.\n",
    "        model.add(GRU(256, dropout=0.9, return_sequences=True))\n",
    "        # Using the intermediate outputs, we pass them to another GRU layer and collect the final output only this time\n",
    "        model.add(GRU(256, dropout=0.9))\n",
    "        # The output is then sent to a fully connected layer that would give us our final output_dim classes\n",
    "        model.add(Dense(output_dimen, activation='softmax'))\n",
    "        # We use the adam optimizer instead of standard SGD since it converges much faster\n",
    "        model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        model.load_weights('./data/triage.HDF5')\n",
    "        model.summary()\n",
    "    print(\"Model weights loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to convert input sentence to model input, and predict result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def findFeatures(text):\n",
    "    textArray = [text]\n",
    "    input_tokenizer = __loadStuff(\"./data/input_tokenizer.p\")\n",
    "    textArray = np.array(pad_sequences(input_tokenizer.texts_to_sequences(textArray), maxlen=maxLength))\n",
    "    return textArray\n",
    "def predictResult(text):\n",
    "    global model, target_reverse_word_index\n",
    "    if model is None:\n",
    "        print(\"Please run \\\"loadModel\\\" first.\")\n",
    "        return None\n",
    "    features = findFeatures(text)\n",
    "    predicted = model.predict(features)[0]\n",
    "    predicted = np.array(predicted)\n",
    "    probab = predicted.max()\n",
    "    predition = target_reverse_word_index[predicted.argmax()+1]\n",
    "    return predition, probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 18, 256)           232960    \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 18, 256)           393984    \n",
      "_________________________________________________________________\n",
      "gru_8 (GRU)                  (None, 256)               393984    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 19)                4883      \n",
      "=================================================================\n",
      "Total params: 1,025,811\n",
      "Trainable params: 1,025,811\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model weights loaded!\n"
     ]
    }
   ],
   "source": [
    "loadModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('dermatology', 0.99926859)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"Skin is quite itchy.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('mouthface', 0.99999321)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"Sore throat fever fatigue.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('back', 0.99995661)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"Lower back hurt, so painful.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('sexualhealth', 0.99996245)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"Very painful with period.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('abdomen', 0.99999595)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictResult(\"Sudden abdominal pain.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
